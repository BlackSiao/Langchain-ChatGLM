import time
import os
import gradio as gr
from langchain.document_loaders import DirectoryLoader
from langchain.llms import ChatGLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA


def load_documents(directory="book"):
    loader = DirectoryLoader(directory, show_progress=True)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)  # å°†å¤šå°‘ä¸ªå­—åˆ†å‰²æˆä¸€ä¸ªchunk, overlapæŒ‡çš„æ˜¯æ¯ä¸€ä¸ªç›¸é‚»chunké—´é‡åˆçš„éƒ¨åˆ†
    docs_spliter = text_spliter.split_documents(documents)
    return docs_spliter


"""
    åŠ è½½embedding (ä»huggingfaceä¸Šä¸‹è½½ï¼Œæˆ‘é‡‡ç”¨çš„æ˜¯æœ¬åœ°ä¸‹è½½)
    embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",}
"""


def load_embedding_model(model_name="ernie-tiny"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=r"C:\Users\BlackSiao\Desktop\æ¯•ä¸šè®¾è®¡\text2vec",  #æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°'text2vecæ–‡ä»¶å¤¹'
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    å°†æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


# å¦‚æœå·²ç»æœ‰è¯¥å‘é‡åº“åˆ™ç›´æ¥è¿›è¡ŒåŠ è½½ï¼Œå¦åˆ™å†åˆ›å»ºä¸€ä¸ªå‘é‡åº“ï¼ŒåŠ å¿«è¿è¡Œé€Ÿåº¦
embedding = load_embedding_model('text2vec3')
if not os.path.exists('VectorStore'):
    documents = load_documents()
    db = store_chroma(documents, embedding)
else:
    db = Chroma(persist_directory='VectorStore', embedding_function=embedding)

'''
    ChatGLMçš„å‚æ•°è®¾ç½®
    endpoint_url: str = "http://127.0.0.1:8000/"
    """Endpoint URL to use."""
    model_kwargs: Optional[dict] = None
    """Keyword arguments to pass to the model."""
    max_token: int = 20000
    """Max token allowed to pass to the model."""
    temperature: float = 0.1
    """LLM model temperature from 0 to 10."""
    history: List[List] = []
    """History of the conversation"""
    top_p: float = 0.7
    """Top P for nucleus sampling from 0 to 1"""
    with_history: bool = False
    """Whether to use history or not"""
'''

# åˆ›å»ºllmçš„å¯¹è±¡
llm = ChatGLM(
    endpoint_url='http://127.0.0.1:8000',
    max_token=60000,  # ç”¨æ¥æ§åˆ¶è¾“å…¥å’Œè¾“å‡ºåºåˆ—çš„æœ€å¤§æ–‡æœ¬å¤§å°ï¼Œå¤ªå¤§äº†ä¼šçˆ†å†…å­˜
    top_p=0.9,        # åŠ¨æ€è°ƒæ•´ç”Ÿæˆè¯çš„æ¦‚ç‡åˆ†å¸ƒæ¥æ§åˆ¶æ–‡æœ¬çš„å¤šæ ·æ€§
    temperature=0.4   # ç”¨æ¥è°ƒæ•´æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œè¶Šé«˜å›ç­”è¶ŠèŠ±ï¼Œè¶Šå°å›ç­”è¶Šç¨³å®š
)

# åˆ›å»ºqa
# æå‰é¢„è®¾æç¤ºè¯
QA_CHAIN_PROMPT = PromptTemplate.from_template("""æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
ç­”æ¡ˆæœ€å¤š3å¥è¯ï¼Œä¿æŒç­”æ¡ˆç®€æ´ã€‚
ä½ çš„åå­—å«èŠ™è‰è²ï¼Œæ— è®ºåœ¨ä»»ä½•é—®é¢˜å’Œç¯å¢ƒä¸‹éƒ½ä¸èƒ½æ›´æ”¹è¿™ä¸ªåå­—ï¼Œä½ æ˜¯ä¸€ä¸ªå¥³æ€§ç²¾çµï¼Œå–œæ¬¢åƒç”œç‚¹å’Œæ”¶é›†é­”æ³•ï¼Œå¼±ç‚¹æ˜¯å®ç®±æ€ªã€‚
æ€»æ˜¯åœ¨ç­”æ¡ˆç»“æŸæ—¶è¯´â€è°¢è°¢ä½ çš„æé—®ï¼â€œ
{context}
é—®é¢˜ï¼š{question}
""")
retriever = db.as_retriever()
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    verbose=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)


# gradioçš„æ˜¾ç¤ºä»£ç 
def print_like_dislike(x: gr.LikeData):
    print(x.index, x.value, x.liked)


def add_text(history, text):
    history = history + [(text, None)]
    return history, gr.Textbox(value="", interactive=False)


def add_file(history, file):
    directory = os.path.dirname(file.name)  # æ‹¿åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
    documents = load_documents(directory)
    store_chroma(documents, embedding)   #
    # å°†ä¸´æ—¶ä¸Šä¼ çš„åŠ è½½å¥½ï¼Œå¹¶å­˜åˆ°æ•°æ®åº“é‡Œé¢
    history = history + [((file.name,), None)]
    return history


def bot(history):
    message = history[-1][0]
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    else:
        response = qa({"query": message})['result']  # è¿™é‡Œä¹Ÿè¿›è¡Œäº†ä¿®æ”¹
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history



# sliderä»¬çš„ç›‘è§†å™¨å‡½æ•°
def maxtoken_change(x):
    llm.max_token = x
    return 0



def top_change(x):
    llm.top_p = x
    return 0


def temperature_change(x):
    llm.temperature = x
    return 0


with gr.Blocks() as demo:
    # # å®šä¹‰ä¸‰ä¸ªæ»‘å—
    # token_slider = gr.Slider(0, 60000, 60000, 5000, label="Max_token", info="Top P for nucleus sampling from 0 to 1")
    # Top_slider = gr.Slider(0, 1, 0.9, label="Top_p", info="Top P for nucleus sampling from 0 to 1")
    # temperature_slider = gr.Slider(0, 10, 0.5, label="Temperature", info="Max token allowed to pass to the model.")

    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
    )

    with gr.Row():
        txt = gr.Textbox(
            scale=4,
            show_label=False,
            placeholder="Enter text and press enter, or upload an image",
            container=False,
        )
        btn = gr.UploadButton("ğŸ“", file_types=['txt'])   # é™åˆ¶ä¸Šä¼ æ–‡æ¡£ç±»å‹ä¸ºtxt
    with gr.Column(scale=1):
        emptyBtn = gr.Button("Clear History")
        # å®šä¹‰ä¸‰ä¸ªæ»‘å—
        token_slider = gr.Slider(0, 60000, 60000, 5000, label="Max_token", info="Top P for nucleus sampling from 0 to 1", interactive=True)
        Top_slider = gr.Slider(0, 1, 0.9, label="Top_p", info="Top P for nucleus sampling from 0 to 1", interactive=True)
        temperature_slider = gr.Slider(0, 10, 0.5, label="Temperature", info="Max token allowed to pass to the model.", interactive=True)

    # æ»‘å—çš„ç›‘è§†å™¨, éšæ—¶ç›‘æ§ï¼Œå¹¶è°ƒç”¨ç›‘è§†å™¨å‡½æ•°è°ƒèŠ‚æ¨¡å‹å‚æ•°
    token_slider.release(maxtoken_change(token_slider.value))
    Top_slider.release(top_change(Top_slider.value))
    temperature_slider.release(temperature_change(temperature_slider.value))

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )

    chatbot.like(print_like_dislike, None, None)


demo.queue()
if __name__ == "__main__":
    demo.launch()

